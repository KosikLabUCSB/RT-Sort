{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective \n",
    "Save data in form needed for semi-artificial (sometimes called synthetic or artificial) data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save traces\n",
    "Stored as .npy and NOT scaled to uV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import h5py\n",
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def _save_traces_mea_old(task):\n",
    "    rec_path, save_path, start_frame, chan_ind, chunk_start, chunk_size, gain = task\n",
    "    sig = h5py.File(rec_path, 'r')['sig']\n",
    "    traces = sig[chan_ind, chunk_start:chunk_start+chunk_size].astype(\"float16\") # * gain\n",
    "    saved_traces = np.load(save_path, mmap_mode=\"r+\")\n",
    "    saved_traces[:, chunk_start-start_frame:chunk_start-start_frame+chunk_size] = traces\n",
    "        \n",
    "def save_traces_mea_old(rec_path, save_path,\n",
    "                        start_ms=0, end_ms=None, samp_freq=20,  # kHz\n",
    "                        default_gain=1,\n",
    "                        chunk_size=100000,\n",
    "                        num_processes=16):\n",
    "    \"\"\"\n",
    "    This only works for the old format of Maxwell MEA .h5 files\n",
    "    \"\"\"\n",
    "    \n",
    "    start_frame = round(start_ms * samp_freq)\n",
    "\n",
    "    recording = h5py.File(rec_path, 'r')\n",
    "\n",
    "    if end_ms is None:\n",
    "        end_frame = recording['sig'].shape[1]\n",
    "    else:\n",
    "        end_frame = round(end_ms * samp_freq)\n",
    "\n",
    "    chan_ind = []\n",
    "    for mapping in recording['mapping']:  # (chan_idx, elec_id, x_cord, y_cord)\n",
    "        if mapping[1] != -1:\n",
    "            chan_ind.append(mapping[0])\n",
    "    if 'lsb' in recording['settings']:\n",
    "        gain = recording['settings']['lsb'][0] * 1e6    \n",
    "    else:\n",
    "        gain = default_gain\n",
    "        print(f\"'lsb' not found in 'settings'. Setting gain to uV to {gain}\")\n",
    "\n",
    "    print(\"Alllocating memory for traces ...\")\n",
    "    traces = np.zeros((len(chan_ind), end_frame-start_frame), dtype=\"float16\")\n",
    "    np.save(save_path, traces)\n",
    "    del traces\n",
    "    \n",
    "    print(\"Extracting traces ...\")\n",
    "    tasks = [(rec_path, save_path, start_frame, chan_ind, chunk_start, chunk_size, gain) \n",
    "             for chunk_start in range(start_frame, end_frame, chunk_size)]\n",
    "    with Pool(processes=num_processes) as pool:\n",
    "        for _ in tqdm(pool.imap_unordered(_save_traces_mea_old, tasks), total=len(tasks)):\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alllocating memory for traces ...\n",
      "Extracting traces ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:32<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alllocating memory for traces ...\n",
      "Extracting traces ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:34<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alllocating memory for traces ...\n",
      "Extracting traces ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:48<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alllocating memory for traces ...\n",
      "Extracting traces ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:35<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alllocating memory for traces ...\n",
      "Extracting traces ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:35<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alllocating memory for traces ...\n",
      "Extracting traces ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:34<00:00,  1.08it/s]\n"
     ]
    }
   ],
   "source": [
    "for rec in ['2950', '2953', '2954', '2957', '5116', '5118']:\n",
    "    save_traces_mea_old(f\"/data/MEAprojects/DLSpikeSorter/data/{rec}/data.raw.h5\", f\"/data/MEAprojects/DLSpikeSorter/data/{rec}/traces.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find TRAINING_MEDIAN needed to run DL model on real recordings\n",
    "For run_dl_model() in RT-Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference base scaling: 12.67\n"
     ]
    }
   ],
   "source": [
    "# For MEA model\n",
    "SAMP_FREQ = 20  # kHz\n",
    "FIRST_MS = 50  # Used to estimate median of first ms\n",
    "##\n",
    "\"\"\"\n",
    "Using IQR because it is ilke standardizing by dividing STD, except IQR is less affected by spikes in first 50ms\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "all_medians = []\n",
    "for rec in ['2950', '2953', '2954', '2957', '5116', '5118']:\n",
    "    traces = np.load(f\"/data/MEAprojects/DLSpikeSorter/data/{rec}/traces.npy\", mmap_mode=\"r\")\n",
    "    window = traces[:, :round(FIRST_MS*SAMP_FREQ)]\n",
    "    iqrs = scipy.stats.iqr(window, axis=1)\n",
    "    median = np.median(iqrs)\n",
    "    all_medians.append(median)\n",
    "print(f\"Inference base scaling: {np.mean(all_medians)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference base scaling: 15.45375\n"
     ]
    }
   ],
   "source": [
    "# For neuropixels model\n",
    "SAMP_FREQ = 30  # kHz\n",
    "FIRST_MS = 50  # Used to estimate median of first ms\n",
    "GAIN_TO_UV = 0.195\n",
    "##\n",
    "\"\"\"\n",
    "Using IQR because it is ilke standardizing by dividing STD, except IQR is less affected by spikes in first 50ms\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "all_medians = []\n",
    "for rec in [\n",
    "    \"/data/MEAprojects/buzsaki/SiegleJ/AllenInstitute_744912849/session_766640955/probe_773592315\",\n",
    "    \"/data/MEAprojects/buzsaki/SiegleJ/AllenInstitute_744912849/session_766640955/probe_773592318\",\n",
    "    \"/data/MEAprojects/buzsaki/SiegleJ/AllenInstitute_744912849/session_766640955/probe_773592320\",\n",
    "    \"/data/MEAprojects/buzsaki/SiegleJ/AllenInstitute_744912849/session_766640955/probe_773592324\",\n",
    "    \"/data/MEAprojects/buzsaki/SiegleJ/AllenInstitute_744912849/session_766640955/probe_773592328\",\n",
    "    \"/data/MEAprojects/buzsaki/SiegleJ/AllenInstitute_744912849/session_766640955/probe_773592330\",\n",
    "]:\n",
    "    traces = np.load(f\"{rec}/traces.npy\", mmap_mode=\"r\")\n",
    "    window = traces[:, :round(FIRST_MS*SAMP_FREQ)] * GAIN_TO_UV\n",
    "    iqrs = scipy.stats.iqr(window, axis=1)\n",
    "    median = np.median(iqrs)\n",
    "    all_medians.append(median)\n",
    "print(f\"Inference base scaling: {np.mean(all_medians)}\")\n",
    "print(\"TRUNCATE THIS NUMBER SO THAT THERE IS ONLY ONE DECIMAL PLACE (DO NOT ROUND)\")\n",
    "print(\"Otherwise, RT-Sort's performance on /data/MEAprojects/primary_mouse/patch_ground_truth/200724/2602/patch_rec_cell7.raw.h5 decreases dramatically (not sure whyy)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale sorted.npz to uV\n",
    "Unit templates need to be in uV in sortd.npz, but older versions have them in the MEA's arbitrary units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAIN_TO_UV = 6.29425\n",
    "##\n",
    "for rec in ['2950', '2953', '2954', '2957', '5116', '5118']:\n",
    "    npz = np.load(f\"/data/MEAprojects/DLSpikeSorter/data/{rec}/sorted.npz\", allow_pickle=True)\n",
    "    npz = dict(npz)\n",
    "    for unit in npz['units']:\n",
    "        unit['template'] *= GAIN_TO_UV\n",
    "        unit['amplitudes'] *= GAIN_TO_UV\n",
    "        # Not need to change 'std_norms' since change to std is cancelled by change to amplitude normalizing the std\n",
    "    np.savez(f\"/data/MEAprojects/DLSpikeSorter/data/{rec}/sorted.npz\", **npz)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "si_dl_ss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
