{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Objective\n",
    "Simulate a real time experiment by replaying an organoid recording and test the speed of DL model detection + prop sorting. For each detection, record the time it took from the spike to occur until the detection and plot these durations as a distribution."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TODO\n",
    "Keep multiple workers alive and break up model outputs to find peaks?\n",
    "Write python code and ask ChatGPT to convert to C++?\n",
    "\n",
    "Maybe ditch the padding on the ends to detect spikes in first and last frame of input if windows overlap\n",
    "\n",
    "End with step function, sum values, conv layer?\n",
    "\n",
    "Break up conv layers so model only outputs 1 value per channel?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# region Set up notebook imports\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "# Allow for imports of other scripts\n",
    "import sys\n",
    "PATH = \"/data/MEAprojects/DLSpikeSorter\"\n",
    "if PATH not in sys.path:\n",
    "    sys.path.append(PATH)\n",
    "# Reload a module after changes have been made\n",
    "from importlib import reload\n",
    "# endregion\n",
    "\n",
    "from time import perf_counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.nn.functional import pad as torch_pad\n",
    "import torch_tensorrt\n",
    "\n",
    "import scipy\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "\n",
    "from src.model import ModelSpikeSorter\n",
    "from src import utils"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "RECORDING = np.load(utils.PATH_REC_DL_NP.format(\"2953\"), mmap_mode=\"r\")\n",
    "MODEL = ModelSpikeSorter.load(\"/data/MEAprojects/DLSpikeSorter/models/v0_4_4/2953/230101_133514_582221\").eval().cuda().to(torch.float16)\n",
    "##\n",
    "loc_prob_thresh_logit = MODEL.loc_prob_thresh_logit\n",
    "logit_to_loc_add = MODEL.loc_first_frame"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setup torch_tensorrt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Start simulation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Simulation pseudocode\n",
    "    detected_spikes = []\n",
    "    sim_time = window_duration  # Simulation clock. 10ms = duration of model window inputs\n",
    "    while sim_time <= recording_duration:\n",
    "        ## Run simulation frame\n",
    "        get numpy window from (sim_time - window_duration to sim_time)\n",
    "\n",
    "        detected_spikes_cur = []\n",
    "\n",
    "        processing_time_start = 0\n",
    "        subtract mean from window\n",
    "        convert window to torch and cuda\n",
    "        get model outputs\n",
    "        get model predictions\n",
    "        plug predictions into prop signal\n",
    "        check if spikes were detected\n",
    "\n",
    "        for every detected spike:\n",
    "            get processing_time_end\n",
    "            processing_time = processing_time_end - processing_time_start\n",
    "            detected_spikes_cur.append(processing_time)\n",
    "\n",
    "        ## Determine detection delay and true/false positive for analysis\n",
    "        for every detected spike in detected_spikes_cur: (in ascending order of spike_time_in_window)\n",
    "            time_spike_was_detected = sim_time + processing_time\n",
    "            predicted_spike_time = sim_time - window_duration + spike_time_in_window\n",
    "\n",
    "            # check if predicted spike matches with a spike in [sim_time - window_duration, sim_time]\n",
    "            # only works because front_buffer_sample and end_buffer_sample > match time (2ms >> 0.4ms)\n",
    "            if closest real spike is close to predicted_spike_time:\n",
    "                true_positive = true\n",
    "                mark real spike as matched\n",
    "            else:\n",
    "                true_positive = false\n",
    "\n",
    "            # delay relative to real spike: detection_delay = time_spike_was_detected - real spike time\n",
    "            # but model predictions times are probably more accurate than kilosort\n",
    "            detection_delay = time_spike_was_detected - predicted_spike_time\n",
    "\n",
    "            detected_spikes.append((true_positive, detection_delay))\n",
    "\n",
    "        ## Continue simulation\n",
    "        sim_time += processing_time\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def warmup(model, inputs, n_runs, verbose=False):\n",
    "    # GPU needs to warmup\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Warming up ...\")\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_runs):\n",
    "            model(inputs.to(\"cuda\"))\n",
    "            torch.cuda.synchronize()\n",
    "warmup(MODEL, torch.tensor(RECORDING[:, None, 1000:1200], dtype=torch.float16), 100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'warmup' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mwarmup\u001B[49m(MODEL, torch\u001B[38;5;241m.\u001B[39mtensor(RECORDING[:, \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m1000\u001B[39m:\u001B[38;5;241m1200\u001B[39m], dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat16, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m), \u001B[38;5;241m100\u001B[39m)\n\u001B[1;32m      3\u001B[0m traces \u001B[38;5;241m=\u001B[39m RECORDING[:, \u001B[38;5;241m2000\u001B[39m:\u001B[38;5;241m2200\u001B[39m]\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# processing_time_start = perf_counter()\u001B[39;00m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'warmup' is not defined"
     ]
    }
   ],
   "source": [
    "warmup(MODEL, torch.tensor(RECORDING[:, None, 1000:1200], dtype=torch.float16, device=\"cuda\"), 100)\n",
    "\n",
    "traces = RECORDING[:, 2000:2200]\n",
    "\n",
    "# processing_time_start = perf_counter()\n",
    "MODEL = MODEL.to(\"cuda\")\n",
    "traces = traces - np.mean(traces, axis=1, keepdims=True)\n",
    "traces = torch.tensor(traces[:, None, :], device=\"cuda\", dtype=torch.float16)\n",
    "with torch.no_grad():\n",
    "    outputs = MODEL(traces)\n",
    "\n",
    "processing_time_start = perf_counter()\n",
    "\n",
    "outputs = outputs.cpu()\n",
    "processing_time_end = perf_counter()\n",
    "\n",
    "(processing_time_end - processing_time_start) * 1000\n",
    "\n",
    "# # outputs = torch_pad(outputs.cpu(), (1, 1), value=-np.inf)\n",
    "# for channel in outputs:\n",
    "#     peaks = find_peaks(channel, height=loc_prob_thresh_logit)\n",
    "# processing_time_end = perf_counter()\n",
    "#\n",
    "# (processing_time_end - processing_time_start) * 1000"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "ModelTuning(\n  (last_layer): Conv1d(50, 1, kernel_size=(21,), stride=(1,))\n  (conv): Sequential(\n    (0): Conv1d(1, 50, kernel_size=(21,), stride=(1,))\n    (1): ReLU()\n    (2): Conv1d(50, 50, kernel_size=(21,), stride=(1,))\n    (3): ReLU()\n    (4): Conv1d(50, 50, kernel_size=(21,), stride=(1,))\n    (5): ReLU()\n    (6): Conv1d(50, 1, kernel_size=(21,), stride=(1,))\n  )\n  (noise): Flatten(start_dim=1, end_dim=-1)\n)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = MODEL.model\n",
    "test.add_module(torch.nn.)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Need to test speed not on juypter notebook"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}