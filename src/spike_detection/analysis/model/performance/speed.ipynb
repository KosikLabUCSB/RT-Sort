{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "Test model speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch_tensorrt\n",
    "\n",
    "from src.model import ModelSpikeSorter\n",
    "from src.utils import random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float16\n",
    "\n",
    "full_model = ModelSpikeSorter.load(\"/data/MEAprojects/DLSpikeSorter/models/v0_4_4/5118/230101_135307_305876\")\n",
    "SAVE_PATH = \"/data/MEAprojects/DLSpikeSorter/models/v0_4_4/model_speed_rerun.npy\"\n",
    "\n",
    "# full_model = ModelSpikeSorter.load(\"/data/MEAprojects/buzsaki/SiegleJ/AllenInstitute_744912849/session_766640955/dl_models/240318/c/240318_165245_967091\")\n",
    "# SAVE_PATH = \"/data/MEAprojects/buzsaki/SiegleJ/AllenInstitute_744912849/session_766640955/dl_models/240318/model_speed.npy\"\n",
    "\n",
    "##\n",
    "model = full_model.model.conv.to(dtype=dtype, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation time vs batch size (num sample elecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [1, 2, 4, 8, 16, 32, 64, 128, 256, 384, 512, 1020, 2048]\n",
    "dtype = torch.float16\n",
    "\n",
    "random_seed(501)\n",
    "\n",
    "# Start testing input sizes\n",
    "\n",
    "def get_inputs(size):\n",
    "    return torch.rand(size, full_model.num_channels_in, full_model.sample_size, dtype=dtype, device=\"cuda\")\n",
    "\n",
    "speed_avgs = []\n",
    "for size in batch_sizes:\n",
    "    # Compile model\n",
    "    model_rt = torch.jit.trace(model, [get_inputs(size)])\n",
    "    model_rt = torch_tensorrt.compile(model_rt, inputs=[torch_tensorrt.Input((size, full_model.num_channels_in, full_model.sample_size), dtype=dtype)], enabled_precisions={dtype})\n",
    "    \n",
    "    # Warm up gpu\n",
    "    with torch.no_grad():\n",
    "        for _ in range(50):\n",
    "            model_rt(get_inputs(size))\n",
    "            torch.cuda.synchronize()\n",
    "            \n",
    "    # Run speed tests\n",
    "    speeds = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(100):\n",
    "            inputs = get_inputs(size)\n",
    "            start_time = time.perf_counter()\n",
    "            model_rt(inputs)\n",
    "            torch.cuda.synchronize()\n",
    "            end_time = time.perf_counter()\n",
    "            speed = (end_time - start_time) * 1000\n",
    "            speeds.append(speed)\n",
    "    speed_avgs.append(np.mean(speeds))\n",
    "    print(f\"{size} samples: {speeds[-1]:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(SAVE_PATH, [batch_sizes, speed_avgs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation time vs input window size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEA model\n",
    "input_sizes = [81, 200]\n",
    "\n",
    "# Neuropixels model\n",
    "\n",
    "dtype = torch.float16\n",
    "\n",
    "random_seed(501)\n",
    "\n",
    "# Start testing input sizes\n",
    "\n",
    "def get_inputs(size):\n",
    "    return torch.rand(size, full_model.num_channels_in, full_model.sample_size, dtype=dtype, device=\"cuda\")\n",
    "\n",
    "speed_avgs = []\n",
    "for size in batch_sizes:\n",
    "    # Compile model\n",
    "    model_rt = torch.jit.trace(model, [get_inputs(size)])\n",
    "    model_rt = torch_tensorrt.compile(model_rt, inputs=[torch_tensorrt.Input((size, full_model.num_channels_in, full_model.sample_size), dtype=dtype)], enabled_precisions={dtype})\n",
    "    \n",
    "    # Warm up gpu\n",
    "    with torch.no_grad():\n",
    "        for _ in range(50):\n",
    "            model_rt(get_inputs(size))\n",
    "            torch.cuda.synchronize()\n",
    "            \n",
    "    # Run speed tests\n",
    "    speeds = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(100):\n",
    "            inputs = get_inputs(size)\n",
    "            start_time = time.perf_counter()\n",
    "            model_rt(inputs)\n",
    "            torch.cuda.synchronize()\n",
    "            end_time = time.perf_counter()\n",
    "            speed = (end_time - start_time) * 1000\n",
    "            speeds.append(speed)\n",
    "    speed_avgs.append(np.mean(speeds))\n",
    "    print(f\"{size} samples: {speeds[-1]:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_ss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
