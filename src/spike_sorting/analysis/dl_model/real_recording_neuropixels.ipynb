{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "We subsequently tested our model on actual spikes detected by Kilosort2 in the hold-out recordings. When only considering the largest waveform amplitude electrodes, we observed precision and recall scores of …±…% and …±…% for organoids and …±…% and …±…% for mice (Sup. Fig. 2A-D). The precision and recall were still …±…% and …±…% for organoids and …±…% and …±…% for mice when we considered the 4th largest waveform amplitude electrodes (Sup. Fig. 2E-H). When interpreting these precision scores, it has to be considered that we are comparing the model against Kilosort2 detections. As such, detections from the model that Kilosort2 misses due to various possible reasons like overlapping waveforms, low amplitude or waveform shape changes will negatively impact the obtained precision scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "%autoreload 2\n",
    "from src.run_alg.model import ModelSpikeSorter\n",
    "from src.comparison import Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run DL model on recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATHS = [\n",
    "    \"/data/MEAprojects/buzsaki/SiegleJ/AllenInstitute_744912849/session_766640955/dl_models/240318/a/240318_161415_981130\",\n",
    "    \"/data/MEAprojects/buzsaki/SiegleJ/AllenInstitute_744912849/session_766640955/dl_models/240318/b/240318_163253_679441\",\n",
    "    \"/data/MEAprojects/buzsaki/SiegleJ/AllenInstitute_744912849/session_766640955/dl_models/240318/c/240318_165245_967091\",\n",
    "    \"/data/MEAprojects/buzsaki/SiegleJ/AllenInstitute_744912849/session_766640955/dl_models/240318/d/240318_172719_805804\",\n",
    "    \"/data/MEAprojects/buzsaki/SiegleJ/AllenInstitute_744912849/session_766640955/dl_models/240318/e/240318_174428_896437\",\n",
    "    \"/data/MEAprojects/buzsaki/SiegleJ/AllenInstitute_744912849/session_766640955/dl_models/240318/f/240318_180745_727120\",\n",
    "]\n",
    "\n",
    "REC_PATHS = [\n",
    "    Path(\"/data/MEAprojects/buzsaki/SiegleJ/AllenInstitute_744912849/session_766640955/probe_773592315\"),\n",
    "    Path(\"/data/MEAprojects/buzsaki/SiegleJ/AllenInstitute_744912849/session_766640955/probe_773592318\"),\n",
    "    Path(\"/data/MEAprojects/buzsaki/SiegleJ/AllenInstitute_744912849/session_766640955/probe_773592320\"),\n",
    "    Path(\"/data/MEAprojects/buzsaki/SiegleJ/AllenInstitute_744912849/session_766640955/probe_773592324\"),\n",
    "    Path(\"/data/MEAprojects/buzsaki/SiegleJ/AllenInstitute_744912849/session_766640955/probe_773592328\"),\n",
    "    Path(\"/data/MEAprojects/buzsaki/SiegleJ/AllenInstitute_744912849/session_766640955/probe_773592330\"),\n",
    "]\n",
    "\n",
    "SAMP_FREQ = 30\n",
    "GAIN_TO_UV = 0.195"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dl_model(model_path, unscaled_traces_path,\n",
    "                 model_traces_path, model_outputs_path,\n",
    "                 device=\"cuda\"):    \n",
    "    \"\"\"\n",
    "    WARNING: [Torch-TensorRT] - Using an engine plan file across different models of devices is not recommended and is likely to affect performance or even cause errors\n",
    "        - This is nothing unless using model on a different GPU than what created it (https://github.com/dusty-nv/jetson-inference/issues/883#issuecomment-754106437)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    np_dtype = \"float16\"\n",
    "    \n",
    "    # region Load model\n",
    "    print(\"Loading DL model ...\")\n",
    "    model = ModelSpikeSorter.load(model_path) \n",
    "    sample_size = model.sample_size\n",
    "    num_output_locs = model.num_output_locs\n",
    "    input_scale = model.input_scale\n",
    "    model = ModelSpikeSorter.load_compiled(model_path)\n",
    "    # endregion\n",
    "    \n",
    "    # region Prepare data\n",
    "    unscaled_traces = np.load(unscaled_traces_path, mmap_mode=\"r\")\n",
    "    \n",
    "    num_chans, rec_duration = unscaled_traces.shape\n",
    "\n",
    "    start_frames_all = np.arange(0, rec_duration-sample_size+1, num_output_locs)\n",
    "    \n",
    "    print(\"Allocating memory to save model traces and outputs ...\")\n",
    "    traces_all = np.zeros(unscaled_traces.shape, dtype=np_dtype)\n",
    "    np.save(model_traces_path, traces_all)\n",
    "    traces_all = np.load(model_traces_path, mmap_mode=\"r+\")\n",
    "\n",
    "    outputs_all = np.zeros((num_chans, start_frames_all.size*num_output_locs), dtype=np_dtype)\n",
    "    np.save(model_outputs_path, outputs_all)\n",
    "    outputs_all = np.load(model_outputs_path, mmap_mode=\"r+\")\n",
    "    # endregion\n",
    "    \n",
    "    # region Calculating inference scaling\n",
    "    # if INFERENCE_SCALING_NUMERATOR is not None:\n",
    "    #     window = scaled_traces[:, :PRE_MEDIAN_FRAMES]\n",
    "    #     iqrs = scipy.stats.iqr(window, axis=1)\n",
    "    #     median = np.median(iqrs)\n",
    "    #     inference_scaling = INFERENCE_SCALING_NUMERATOR / median\n",
    "    # else:\n",
    "    #     inference_scaling = 1\n",
    "    inference_scaling = GAIN_TO_UV\n",
    "    print(f\"Inference scaling: {inference_scaling}\")\n",
    "    # endregion\n",
    "    \n",
    "    # region Run model\n",
    "    print(\"Running model ...\")    \n",
    "    with torch.no_grad():\n",
    "        for start_frame in tqdm(start_frames_all):\n",
    "            traces_torch = torch.tensor(unscaled_traces[:, start_frame:start_frame+sample_size], device=device, dtype=torch.float16)\n",
    "            traces_torch -= torch.median(traces_torch, dim=1, keepdim=True).values\n",
    "            outputs = model(traces_torch[:, None, :] * input_scale * inference_scaling).cpu()\n",
    "\n",
    "            traces_all[:, start_frame:start_frame+sample_size] = traces_torch.cpu()\n",
    "            outputs_all[:, start_frame:start_frame+num_output_locs] = outputs[:, 0, :]\n",
    "    # endregion\n",
    "    \n",
    "    # region Save traces and outputs\n",
    "    # np.save(model_traces_path, traces_all)\n",
    "    # np.save(model_outputs_path, outputs_all)\n",
    "    # endregion\n",
    "    \n",
    "def extract_crossings(model_outputs_path, \n",
    "                      all_crossings_path, elec_crossings_ind_path,\n",
    "                      front_buffer, loc_prob_thresh_logit,\n",
    "                      end_ms=None,\n",
    "                      window_size=1000,\n",
    "                      device=\"cpu\"):          \n",
    "    outputs = np.load(model_outputs_path, mmap_mode=\"r\")\n",
    "    num_elecs = outputs.shape[0]\n",
    "    \n",
    "    # Using end_ms (rel to recording traces) since only need crossings during pre-recording to form sequences, not entire duration  \n",
    "    if end_ms is None:\n",
    "        end_frame = outputs.shape[1]-window_size-1\n",
    "    else: \n",
    "        end_frame = round(end_ms * SAMP_FREQ) - front_buffer  # -FRONT_BUFFER to convert from rec. traces to model outputs\n",
    "    \n",
    "    all_crossings = []  # [(elec_idx, time, amp)]\n",
    "    elec_crossings_ind = [[] for _ in range(num_elecs)]  # ith element for elec idx i. Contains ind in all_crossings for elec idx i's crossings\n",
    "    crossing_idx = 0\n",
    "    for start_frame in tqdm(range(0, end_frame, window_size)):\n",
    "        if start_frame >= end_frame: \n",
    "            break\n",
    "        \n",
    "        window = outputs[:, start_frame:start_frame+window_size+2]\n",
    "        window = torch.tensor(window, device=device)\n",
    "        \n",
    "        main = window[:, 1:-1]\n",
    "        greater_than_left = main > window[:, :-2]\n",
    "        greater_than_right = main > window[:, 2:]\n",
    "        peaks = greater_than_left & greater_than_right\n",
    "        crosses = main >= loc_prob_thresh_logit\n",
    "        nonzeros = torch.nonzero((peaks & crosses).T)  # .T is so that outputs are ordered based on peak_ind first then elec_ind\n",
    "        for peak, elec in nonzeros:\n",
    "            peak = peak.item()\n",
    "            elec = elec.item()\n",
    "            time_ms = (front_buffer + peak + start_frame + 1) / SAMP_FREQ  # +1 since rel. to main (which is +1 rel to window and window is rel. to start_frame)\n",
    "            all_crossings.append((elec, time_ms, -1)) \n",
    "            elec_crossings_ind[elec].append(crossing_idx)\n",
    "            crossing_idx += 1\n",
    "            \n",
    "        # if start_frame > 10000:\n",
    "        #     times = [all_crossings[idx][1] for idx in elec_crossings_ind[17]]\n",
    "        #     print(times)\n",
    "        #     plot_spikes(times, 17)\n",
    "        #     plt.show()\n",
    "        #     return\n",
    "    \n",
    "    np.save(all_crossings_path, np.array(all_crossings, dtype=object))\n",
    "    np.save(elec_crossings_ind_path, np.array(elec_crossings_ind, dtype=object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting on probe_773592315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [Torch-TensorRT] - For input input.1, found user specified input dtype as Float16. The compiler is going to use the user setting Float16\n",
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.3.2\n",
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.3.2\n",
      "WARNING: [Torch-TensorRT] - TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.3.2\n",
      "WARNING: [Torch-TensorRT] - TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.3.2\n",
      "WARNING: [Torch-TensorRT] - TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.3.2\n",
      "WARNING: [Torch-TensorRT] - TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.3.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DL model ...\n",
      "Allocating memory to save model traces and outputs ...\n",
      "Inference scaling: 0.195\n",
      "Running model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 199999/199999 [16:04<00:00, 207.29it/s] \n",
      "100%|██████████| 35999/35999 [02:00<00:00, 298.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting on probe_773592318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [Torch-TensorRT] - For input input.1, found user specified input dtype as Float16. The compiler is going to use the user setting Float16\n",
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.3.2\n",
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.3.2\n",
      "WARNING: [Torch-TensorRT] - TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.3.2\n",
      "WARNING: [Torch-TensorRT] - TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.3.2\n",
      "WARNING: [Torch-TensorRT] - TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.3.2\n",
      "WARNING: [Torch-TensorRT] - TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.3.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DL model ...\n",
      "Allocating memory to save model traces and outputs ...\n",
      "Inference scaling: 0.195\n",
      "Running model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 199999/199999 [16:15<00:00, 204.99it/s]\n",
      "100%|██████████| 35999/35999 [03:17<00:00, 181.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting on probe_773592320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [Torch-TensorRT] - For input input.1, found user specified input dtype as Float16. The compiler is going to use the user setting Float16\n",
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.3.2\n",
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.3.2\n",
      "WARNING: [Torch-TensorRT] - TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.3.2\n",
      "WARNING: [Torch-TensorRT] - TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.3.2\n",
      "WARNING: [Torch-TensorRT] - TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.3.2\n",
      "WARNING: [Torch-TensorRT] - TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.3.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DL model ...\n",
      "Allocating memory to save model traces and outputs ...\n",
      "Inference scaling: 0.195\n",
      "Running model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 199999/199999 [09:21<00:00, 356.27it/s]\n",
      "100%|██████████| 35999/35999 [01:42<00:00, 350.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting on probe_773592324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [Torch-TensorRT] - For input input.1, found user specified input dtype as Float16. The compiler is going to use the user setting Float16\n",
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.3.2\n",
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.3.2\n",
      "WARNING: [Torch-TensorRT] - TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.3.2\n",
      "WARNING: [Torch-TensorRT] - TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.3.2\n",
      "WARNING: [Torch-TensorRT] - TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.3.2\n",
      "WARNING: [Torch-TensorRT] - TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.3.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DL model ...\n",
      "Allocating memory to save model traces and outputs ...\n",
      "Inference scaling: 0.195\n",
      "Running model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 199999/199999 [12:53<00:00, 258.44it/s]\n",
      "100%|██████████| 35999/35999 [01:29<00:00, 402.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting on probe_773592328\n",
      "Loading DL model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [Torch-TensorRT] - For input input.1, found user specified input dtype as Float16. The compiler is going to use the user setting Float16\n",
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.3.2\n",
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.3.2\n",
      "WARNING: [Torch-TensorRT] - TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.3.2\n",
      "WARNING: [Torch-TensorRT] - TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.3.2\n",
      "WARNING: [Torch-TensorRT] - TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.3.2\n",
      "WARNING: [Torch-TensorRT] - TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.3.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocating memory to save model traces and outputs ...\n",
      "Inference scaling: 0.195\n",
      "Running model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 199999/199999 [09:06<00:00, 366.14it/s]\n",
      "100%|██████████| 35999/35999 [02:55<00:00, 204.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting on probe_773592330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [Torch-TensorRT] - For input input.1, found user specified input dtype as Float16. The compiler is going to use the user setting Float16\n",
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.3.2\n",
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.3.2\n",
      "WARNING: [Torch-TensorRT] - TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.3.2\n",
      "WARNING: [Torch-TensorRT] - TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.3.2\n",
      "WARNING: [Torch-TensorRT] - TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.3.2\n",
      "WARNING: [Torch-TensorRT] - TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.3.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DL model ...\n",
      "Allocating memory to save model traces and outputs ...\n",
      "Inference scaling: 0.195\n",
      "Running model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 199999/199999 [09:15<00:00, 360.16it/s] \n",
      "100%|██████████| 35999/35999 [01:45<00:00, 340.32it/s]\n"
     ]
    }
   ],
   "source": [
    "for model_path, rec_path in zip(MODEL_PATHS, REC_PATHS):\n",
    "    model_path = Path(model_path)\n",
    "    \n",
    "    print(f\"Starting on {rec_path.name}\")\n",
    "    inter_path = model_path / \"run_on_holdout_recording\"\n",
    "    inter_path.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    unscaled_traces_path = rec_path / \"traces.npy\"\n",
    "    \n",
    "    model_traces_path = inter_path / \"model_traces.npy\"\n",
    "    model_outputs_path = inter_path / \"model_outputs.npy\"\n",
    "    \n",
    "    all_crossings_path = inter_path / \"all_crossings.npy\"\n",
    "    elec_crossings_ind_path = inter_path / \"elec_crossings_ind.npy\"\n",
    "    \n",
    "    model = ModelSpikeSorter.load(model_path)\n",
    "    num_elecs = np.load(unscaled_traces_path, mmap_mode=\"r\").shape[0]\n",
    "    model.compile(num_elecs, model_path)    \n",
    "    \n",
    "    run_dl_model(model_path, unscaled_traces_path, model_traces_path, model_outputs_path)\n",
    "    extract_crossings(model_outputs_path, all_crossings_path, elec_crossings_ind_path,\n",
    "                      front_buffer=model.buffer_front_sample, loc_prob_thresh_logit=model.loc_prob_thresh_logit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Pseudocode:\n",
    "Save per model/recording\n",
    "\n",
    "For each unit, get template.\n",
    "For 0...5 highest amp electrode, count number of matches\n",
    "Return [num_true_positives, num_false_postiives, num_false_negatives for each highest amp electrode]\n",
    "Store all in array\n",
    "\n",
    "[num_units, 6=(tested electrodes), 3=(num_true_postives, num_false_positives, num_false_negatives)]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting on probe_773592315\n",
      "Total num model detections: 15778671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/220 [00:00<?, ?it/s]/home/mea/anaconda3/envs/si_dl_ss/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/mea/anaconda3/envs/si_dl_ss/lib/python3.7/site-packages/numpy/core/_methods.py:182: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      " 65%|██████▍   | 142/220 [01:15<00:27,  2.86it/s]/home/mea/anaconda3/envs/si_dl_ss/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/mea/anaconda3/envs/si_dl_ss/lib/python3.7/site-packages/numpy/core/_methods.py:182: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "100%|██████████| 220/220 [01:37<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting on probe_773592318\n",
      "Total num model detections: 16609692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 123/257 [01:12<00:51,  2.62it/s]/home/mea/anaconda3/envs/si_dl_ss/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/mea/anaconda3/envs/si_dl_ss/lib/python3.7/site-packages/numpy/core/_methods.py:182: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "100%|██████████| 257/257 [02:01<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting on probe_773592320\n",
      "Total num model detections: 12479824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 340/340 [02:06<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting on probe_773592324\n",
      "Total num model detections: 8671967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 186/186 [01:22<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting on probe_773592328\n",
      "Total num model detections: 21499889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 389/389 [02:29<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting on probe_773592330\n",
      "Total num model detections: 12397703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 43/198 [00:41<00:46,  3.36it/s] /home/mea/anaconda3/envs/si_dl_ss/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/mea/anaconda3/envs/si_dl_ss/lib/python3.7/site-packages/numpy/core/_methods.py:182: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      " 72%|███████▏  | 142/198 [01:12<00:13,  4.19it/s]/home/mea/anaconda3/envs/si_dl_ss/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/mea/anaconda3/envs/si_dl_ss/lib/python3.7/site-packages/numpy/core/_methods.py:182: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "100%|██████████| 198/198 [01:28<00:00,  2.23it/s]\n"
     ]
    }
   ],
   "source": [
    "X_HIGHEST = 5  # Test up to X_HIGHEST electrode (max electrode = 0)\n",
    "\n",
    "# For extracting unit template to determine highest elecs\n",
    "N_BEFORE = N_AFTER = round(0.5 * SAMP_FREQ)\n",
    "N_BEFORE_MEDIAN = N_AFTER_MEDIAN = N_AFTER_MEDIAN = round(50*SAMP_FREQ)\n",
    "NUM_SPIKES_ESTIMATOR = 300  # Number of spikes needed to determine which of electrodes have largest amp in unit's template\n",
    "\n",
    "def job(ks_unit_id):\n",
    "    ks_spike_frames = KS_SPIKE_TIMES[KS_SPIKE_CLUSTERS == ks_unit_id]\n",
    "    # Find amplitudes on elecs\n",
    "    np.random.seed(231)\n",
    "    templates = np.zeros([TRACES.shape[0], N_BEFORE+N_AFTER+1], dtype=float)\n",
    "    for frame in np.random.choice(ks_spike_frames, NUM_SPIKES_ESTIMATOR):\n",
    "        if frame < N_BEFORE:  # Prevent broadcasting problems\n",
    "            continue\n",
    "        window = TRACES[:, frame-N_BEFORE:frame+N_AFTER+1] - np.median(TRACES[:, frame-N_BEFORE_MEDIAN:frame+N_AFTER_MEDIAN+1], axis=1, keepdims=True)\n",
    "        templates += window\n",
    "    amps = np.max(np.abs(templates), axis=1)\n",
    "    highest_elecs = np.argsort(-amps)\n",
    "    ks_spike_times = ks_spike_frames / SAMP_FREQ\n",
    "\n",
    "    perfs = []\n",
    "    for elec in highest_elecs[:X_HIGHEST+1]:\n",
    "        model_times = [ALL_CROSSINGS[idx][1] for idx in ELEC_CROSSINGS_IND[elec]]\n",
    "        num_tp = Comparison.count_matching_events(model_times, ks_spike_times)\n",
    "        perfs.append((num_tp, len(model_times)-num_tp, len(ks_spike_times)-num_tp))\n",
    "    return perfs\n",
    "\n",
    "for model_path, rec_path in zip(MODEL_PATHS, REC_PATHS):\n",
    "    model_path = Path(model_path)\n",
    "    \n",
    "    print(f\"Starting on {rec_path.name}\")\n",
    "    inter_path = model_path / \"run_on_holdout_recording\"    \n",
    "    unscaled_traces_path = rec_path / \"traces.npy\"\n",
    "    all_crossings_path = inter_path / \"all_crossings.npy\"\n",
    "    elec_crossings_ind_path = inter_path / \"elec_crossings_ind.npy\"\n",
    "    \n",
    "    TRACES = np.load(unscaled_traces_path, mmap_mode=\"r\")\n",
    "    ALL_CROSSINGS = np.load(all_crossings_path, allow_pickle=True)\n",
    "    ELEC_CROSSINGS_IND = np.load(elec_crossings_ind_path, allow_pickle=True)\n",
    "    \n",
    "    KS_SPIKE_TIMES = np.load(rec_path / \"spikesort_matlab4/curation/first/spike_times.npy\")\n",
    "    KS_SPIKE_CLUSTERS = np.load(rec_path / \"spikesort_matlab4/curation/first/spike_clusters.npy\")\n",
    "    \n",
    "    print(f\"Total num model detections: {len(ALL_CROSSINGS)}\") # Sanity check that there are model crossings\n",
    "    \n",
    "    ks_unit_ids = np.unique(KS_SPIKE_CLUSTERS)\n",
    "    \n",
    "    with Pool(processes=16) as pool:\n",
    "        all_perfs = []\n",
    "        for perfs in tqdm(pool.imap(job, ks_unit_ids), total=len(ks_unit_ids)):\n",
    "            all_perfs.append(perfs)\n",
    "        all_perfs = np.array(all_perfs)\n",
    "    np.save(inter_path / \"units_highest_elecs_tp_fp_fn.npy\", all_perfs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "si_dl_ss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
