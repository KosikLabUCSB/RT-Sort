{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "Compare RT-Sort to other sorters by:\n",
    "1. Pair units based on Hungarian match overlap score\n",
    "2. For each pair:\n",
    "   1. Store RT-Sort pre-recording spikes mean and STD latency and amplitude on all elecs\n",
    "   2. Find spikes detected by both, RT-Sort only, other sorter only\n",
    "   3. Plot and save mean footprints\n",
    "   4. For each group\n",
    "       1. For each spike:\n",
    "           1. Find latency and amplitude on all elecs\n",
    "\n",
    "Data will be stored as:\\\n",
    "One file per sorter, representing RT-Sort compared to that sorter. File will be list where element is a dictionary. The dictionary represents an RT-Sort sequence. It has the items:\n",
    "- \"idx\": Index of sequence in footprint plots (which is the index of the sequence if all sequences were in one list). Sequences with less than 2 spikes are not included since 2 spikes are needed for calculating the STD\n",
    "- \"location\": (x, y) location of sequence's root electrode\n",
    "- \"root_elec\": Index of root electrode\n",
    "- \"inner_loose_elecs\": Indices of inner loose electrodes (including root)\n",
    "- \"loose_elecs\": Indices of loose electrodes (including root)\n",
    "- \"footprint_elecs\": Indices of footprint electrodes (including root)\n",
    "- \"means\": Numpy array with shape (num_elecs, 2) where each row represents an electrode. The columns are (starting with column 0):\n",
    "  - Mean latency\n",
    "  - Mean amplitude\n",
    "- \"stds\": Same as \"means\" except the STD rather than the mean\n",
    "  \n",
    "- \"other_unit_idx:\" Index of the other unit\n",
    "- \"overlap_score\": Overlap score between RT-Sort sequence and other sorter's unit.\n",
    "  \n",
    "- \"matching_spikes\": List where each element is a dictionary that represents a spike detected by both RT-Sort and other sorter.\n",
    "  - \"time\": Time of spike in milliseconds, relative to start of the recording (not relative to the testing 5-10 minutes region)\n",
    "  - \"elecs\": Numpy array with shape (num_elecs, 2) where each row represents an electrode. The columns are (starting with column 0):\n",
    "    - Spike's latency\n",
    "    - Spike's amplitude\n",
    "- \"rt_sort_only_spikes\": Same as \"matching_spikes\" but spikes only detected by RT-Sort\n",
    "- \"other_sorter_only_spikes\": Same as \"matching_spikes\" but spikes only detected by the other sorter\n",
    "   \n",
    "240220 - RT-Sort is definitely undermerging and misses many of other sorter's spikes while having consensus spikes, so using score_formula=#matches/#rt_sort and each unit from one sorter can be paired with more than one unit from other sorter\n",
    "\n",
    "*Note: \"amplitude\" refers to standardized amplitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globals setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from spikeinterface.extractors import NpzSortingExtractor\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "%autoreload 2\n",
    "from src.comparison import Comparison, DummySorter\n",
    "from src import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mea/anaconda3/envs/si_dl_ss/lib/python3.7/site-packages/hdmf/spec/namespace.py:532: UserWarning: Ignoring cached namespace 'hdmf-common' version 1.1.3 because version 1.6.0 is already loaded.\n",
      "  % (ns['name'], ns['version'], self.__namespaces.get(ns['name'])['version']))\n",
      "/home/mea/anaconda3/envs/si_dl_ss/lib/python3.7/site-packages/hdmf/spec/namespace.py:532: UserWarning: Ignoring cached namespace 'core' version 2.2.5 because version 2.5.0 is already loaded.\n",
      "  % (ns['name'], ns['version'], self.__namespaces.get(ns['name'])['version']))\n",
      "/home/mea/anaconda3/envs/si_dl_ss/lib/python3.7/site-packages/pynwb/ecephys.py:93: UserWarning: ElectricalSeries 'ElectricalSeries': The second dimension of data does not match the length of electrodes. Your data may be transposed.\n",
      "  \"Your data may be transposed.\" % (self.__class__.__name__, kwargs[\"name\"]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording does not have scaled traces. Setting gain to 0.195\n"
     ]
    }
   ],
   "source": [
    "SORTERS_ROOT = Path(\"/data/MEAprojects/spikeinterface/spiketrains/mouse412804_probeC\")\n",
    "TRACES = np.load(\"/data/MEAprojects/dandi/000034/sub-mouse412804/rt_sort/dl_model/240318/scaled_traces.npy\", mmap_mode=\"r\")  \n",
    "SEQUENCES_PATH = Path(\"/data/MEAprojects/dandi/000034/sub-mouse412804/rt_sort/240319/tested_sequences.pickle\")\n",
    "ELEC_LOCS = utils.rec_si().get_channel_locations()\n",
    "SAMP_FREQ = 30  # kHz\n",
    "N_BEFORE = N_AFTER = round(0.5 * SAMP_FREQ)  # round(0.3 * SAMP_FREQ)\n",
    "PRE_MEDIAN_FRAMES = round(50 * SAMP_FREQ)  # If None, use entire recording for SNR\n",
    "MAX_FR = 1000  # for units, Hz\n",
    "\n",
    "TIME_FRAME = (5*60*1000, min(10*60*1000, TRACES.shape[1]/SAMP_FREQ))  # start_ms, end_ms\n",
    "\n",
    "##\n",
    "SEQUENCES = utils.pickle_load(SEQUENCES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sorter:\n",
    "    # Wrapper of NpzSortingExtractor for Comparison\n",
    "    def __init__(self, full_name, name):\n",
    "        self.npz = NpzSortingExtractor(SORTERS_ROOT / full_name / \"sorting_cached.npz\")\n",
    "        self.name = name\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.get_spike_times()) \n",
    "    \n",
    "    def get_spike_times(self):\n",
    "        start_ms, end_ms = TIME_FRAME\n",
    "        \n",
    "        spike_times = []\n",
    "        for uid in self.npz.get_unit_ids():\n",
    "            times = self.npz.get_unit_spike_train(uid) / SAMP_FREQ\n",
    "            times_ind = start_ms <= times\n",
    "            times_ind *= times <= end_ms\n",
    "            if MAX_FR * (end_ms - start_ms)/1000 >= sum(times_ind) > 0:\n",
    "                spike_times.append(times[times_ind])\n",
    "        return spike_times\n",
    "    \n",
    "HS = Sorter(\"herdingspikes\", \"herdingspikes\")\n",
    "KS = Sorter(\"kilosort2\", \"kilosort2\")\n",
    "IC = Sorter(\"ironclust\", \"ironclust\")\n",
    "TDC = Sorter(\"tridesclous\", \"tridesclous\")\n",
    "SC = Sorter(\"spykingcircus\", \"spykingcircus\")\n",
    "HDS = Sorter(\"hdsort\", \"hdsort\")\n",
    "OTHER_SORTERS = [KS, HS, IC, TDC, SC, HDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 248/248 [00:04<00:00, 51.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# If using SNR across entire recording, need to calculate that before running next cell\n",
    "def _calc_snr(chan):\n",
    "    return chan, np.clip(np.median(np.abs(TRACES[chan, :])) / 0.6745, a_min=0.5, a_max=None)  # clip to prevent divide by 0\n",
    "\n",
    "if PRE_MEDIAN_FRAMES is None:\n",
    "    tasks = range(TRACES.shape[0])\n",
    "    PRE_MEDIANS = np.zeros(len(tasks), dtype=float)\n",
    "    with Pool(processes=16) as pool:\n",
    "        for chan, snr in tqdm(pool.imap_unordered(_calc_snr, tasks), total=len(tasks)):\n",
    "            PRE_MEDIANS[chan] = snr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def job(task):\n",
    "    seq_idx, other_sorter_idx = task\n",
    "    seq = SEQUENCES[seq_idx]\n",
    "    seq_spike_train = seq.spike_train\n",
    "    if len(seq_spike_train) < 2:\n",
    "        return None\n",
    "    if PRE_MEDIAN_FRAMES is None:\n",
    "        pre_medians = PRE_MEDIANS\n",
    "\n",
    "    save_data = {\n",
    "        \"idx\": seq.idx,\n",
    "        \"location\": ELEC_LOCS[seq.root_elec],\n",
    "        \"root_elec\": seq.root_elec,\n",
    "        \"inner_loose_elecs\": seq.inner_loose_elecs,\n",
    "        \"loose_elecs\": seq.loose_elecs,\n",
    "        \"footprint_elecs\": seq.comp_elecs,\n",
    "        \"means\": np.vstack((\n",
    "            seq.all_latencies,\n",
    "            seq.all_amp_medians\n",
    "        )).T,\n",
    "        \"stds\": np.vstack((\n",
    "            np.std(seq.every_latency, axis=1, ddof=1),\n",
    "            np.std(seq.every_amp_median, axis=1, ddof=1)\n",
    "        )).T    \n",
    "\n",
    "    } \n",
    "    other_sorter = OTHER_SORTERS[other_sorter_idx]\n",
    "    best_score = -np.inf\n",
    "    best_idx = None\n",
    "    best_spike_trains = None  # Will be tuple of (matching, unmatching RT-Sort, unmatching other sorter)\n",
    "    for other_idx, other_spike_train in enumerate(other_sorter.get_spike_times()):\n",
    "        matching, seq_only, other_only = Comparison.get_matching_events(seq_spike_train, other_spike_train)\n",
    "        score = len(matching) / (len(matching) + len(seq_only) + len(other_only))\n",
    "        assert 0 <= score <= 1, \"overlap score is calculated wrong\"\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_idx = other_idx\n",
    "            best_spike_trains = (matching, seq_only, other_only)\n",
    "    save_data[\"other_unit_idx\"] = best_idx\n",
    "    save_data[\"overlap_score\"] = best_score\n",
    "    \n",
    "    array_elecs = np.arange(ELEC_LOCS.shape[0])  # for indexing so latency time matches with amplitude\n",
    "    for name, spike_train in zip((\"matching_spikes\", \"rt_sort_only_spikes\", \"other_sorter_only_spikes\"), best_spike_trains):\n",
    "        spike_data = []\n",
    "        for spike in spike_train:\n",
    "            # Get spike window\n",
    "            frame = round(spike * SAMP_FREQ)\n",
    "            this_n_before = N_BEFORE if frame - N_BEFORE >= 0 else frame  # Prevents indexing negative value as start\n",
    "            rec_window = np.abs(TRACES[:, frame-this_n_before:frame+N_AFTER+1])\n",
    "            \n",
    "            # Get latencies\n",
    "            latencies = np.argmax(rec_window, axis=1) - this_n_before\n",
    "            if name != \"other_sorter_only_spikes\":\n",
    "                frame_offset = 0  # Frame of spike is frame + frame_offset\n",
    "                latencies[seq.root_elec] = 0\n",
    "            else:\n",
    "                frame_offset = latencies[seq.root_elec]\n",
    "                latencies -= latencies[seq.root_elec]\n",
    "            assert latencies[seq.root_elec] == 0, \"Root electrode has a non-zero latency\"\n",
    "            \n",
    "            # Get pre-medians\n",
    "            if PRE_MEDIAN_FRAMES is not None:\n",
    "                pre_medians = TRACES[:, max(0, frame-this_n_before-PRE_MEDIAN_FRAMES):frame-this_n_before]\n",
    "                pre_medians = np.median(np.abs(pre_medians), axis=1)\n",
    "                pre_medians = np.clip(pre_medians / 0.6745, a_min=0.5, a_max=None)  # Prevents median of 0\n",
    "            \n",
    "            # Get amps\n",
    "            # amps = rec_window[array_elecs, this_n_before + latencies]  # index out of bounds error when accounting for other sorter spikes not being centered on root elec trough\n",
    "            amps = np.abs(TRACES[array_elecs, frame+frame_offset+latencies])\n",
    "            amps = amps / pre_medians \n",
    "            \n",
    "            # Store data\n",
    "            spike_data.append({\n",
    "                \"time\": spike,\n",
    "                \"elecs\": np.vstack((latencies, amps)).T\n",
    "            })\n",
    "        save_data[name] = spike_data\n",
    "    \n",
    "    return save_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kilosort2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 168/168 [00:58<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "herdingspikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 168/168 [00:26<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ironclust\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 168/168 [02:40<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tridesclous\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 168/168 [00:50<00:00,  3.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spykingcircus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 168/168 [02:12<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdsort\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 168/168 [01:19<00:00,  2.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'divide': 'ignore', 'over': 'ignore', 'under': 'ignore', 'invalid': 'ignore'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVE_ROOT = Path(SEQUENCES_PATH.parent / \"consistent_spikes/prop_delay_0.5ms/snr_entire_recording\")\n",
    "SAVE_ROOT.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "np.seterr(\"ignore\")\n",
    "##  \n",
    "for sorter_idx in range(len(OTHER_SORTERS)):\n",
    "    sorter_name = OTHER_SORTERS[sorter_idx].name\n",
    "    print(sorter_name)\n",
    "    all_save_data = []\n",
    "    tasks = [(idx, sorter_idx) for idx in range(len(SEQUENCES))]\n",
    "    with Pool(processes=16) as pool:\n",
    "        for save_data in tqdm(pool.imap_unordered(job, tasks), total=len(tasks)):\n",
    "            if save_data is not None:\n",
    "                all_save_data.append(save_data)\n",
    "    np.save(SAVE_ROOT / f\"{sorter_name}.npy\", np.array(all_save_data, dtype=object))\n",
    "np.seterr(\"raise\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
